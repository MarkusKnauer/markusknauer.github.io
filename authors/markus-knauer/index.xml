<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Markus Knauer</title>
    <link>https://markusknauer.github.io/authors/markus-knauer/</link>
    <description>Recent content on Markus Knauer</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Apr 2025 08:59:00 +0200</lastBuildDate>
    <atom:link href="https://markusknauer.github.io/authors/markus-knauer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title> *ICRA Paper*: &#34;Grounding Embodied Question-Answering with State Summaries from Existing Robot Modules&#34;  </title>
      <link>https://markusknauer.github.io/posts/icra-rag-embodied-qa/</link>
      <pubDate>Mon, 28 Apr 2025 08:59:00 +0200</pubDate>
      <guid>https://markusknauer.github.io/posts/icra-rag-embodied-qa/</guid>
      <description>&lt;p&gt;Our approach for &lt;strong&gt;truthful&lt;/strong&gt; &lt;strong&gt;foundation model&lt;/strong&gt; (&lt;strong&gt;LLM&lt;/strong&gt;) question-answering using &lt;em&gt;RAG&lt;/em&gt; methods in &lt;strong&gt;robotics&lt;/strong&gt; was accepted by &lt;strong&gt;IEEE ICRA&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;&#xA;      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/OX-dd5PiiZo?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;&#xA;    &lt;/div&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Paper on IEEE *coming soon!&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://elib.dlr.de/205203/%22&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on Elib&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://youtu.be/OX-dd5PiiZo?si=oB5V8kO_cFpCnSgW&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube Link&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title> *RA-L Paper*: &#34;Interactive Incremental Learning of Generalizable Skills With Local Trajectory Modulation&#34; </title>
      <link>https://markusknauer.github.io/posts/ral-interactive-incremental-learning/</link>
      <pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://markusknauer.github.io/posts/ral-interactive-incremental-learning/</guid>
      <description>&lt;p&gt;Our approach on probabilistic machine learning method for &lt;strong&gt;interactive robot skill modulation&lt;/strong&gt; using a &lt;strong&gt;task-parameterized,&lt;/strong&gt; &lt;strong&gt;kernelized&lt;/strong&gt; method (TP-KMP) got published in IEEE Robotics and Automation Letters (&lt;strong&gt;RA-L&lt;/strong&gt;).&lt;/p&gt;&#xA;&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;&#xA;      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/nqigz0l1syA?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;&#xA;    &lt;/div&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10887119/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on IEEE&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.05655&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on ArXiv&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://elib.dlr.de/212796/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on Elib&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://youtu.be/nqigz0l1syA&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube Link&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;For more information please follow the link to the GitHub project:&lt;/p&gt;</description>
    </item>
    <item>
      <title>*Demonstration*: Showing a task-parameterized approach at Automatica and Hannover fair </title>
      <link>https://markusknauer.github.io/posts/automatica-hannover-demo/</link>
      <pubDate>Mon, 22 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://markusknauer.github.io/posts/automatica-hannover-demo/</guid>
      <description>&lt;h1 id=&#34;automatica-2023--hannover-fair-2024&#34;&gt;&#xA;  Automatica 2023 &amp;amp; Hannover fair 2024&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#automatica-2023--hannover-fair-2024&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;picture class=&#34;demo-image&#34;&gt;&#xA;  &lt;source srcset=&#34;https://markusknauer.github.io/images/tradefair.webp&#34; type=&#34;image/webp&#34;&gt;&#xA;  &lt;img src=&#34;https://markusknauer.github.io/images/tradefair.jpeg&#34; alt=&#34;Robotics demonstration at Automatica and Hannover Fair&#34; loading=&#34;lazy&#34;&gt;&#xA;&lt;/picture&gt;&#xA;&lt;p&gt;Presenting a robotic demonstrator at a trade fair is always exciting! ðŸš€&lt;/p&gt;&#xA;&lt;p&gt;At &lt;a href=&#34;https://factory-of-the-future.dlr.de/2023/05/31/27-30-juni-2023-automatica-2023/index.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Automatica 2023&lt;/a&gt; in Munich and &lt;a href=&#34;https://www.dlr.de/de/aktuelles/nachrichten/2024/das-dlr-auf-der-hannover-messe-2024&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hannover fair&lt;/a&gt; 2024 in Hannover I presented an implementation of a Task-Parameterized approach (TP-GMM [1])&#xA;at the booth of the &lt;a href=&#34;https://www.dlr.de/rm&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institute of Robotics and Mechatronics&lt;/a&gt; of the &lt;a href=&#34;https://www.dlr.de&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;German Aerospace Center (DLR)&lt;/a&gt; in the domain of future manufacturing.&#xA;The demonstration showed a ring-measuring task, where the robot arm picks a bearing ring from a box and places it onto a measuring device. Thanks to the task-parameterized approach a generalization to different box and measuring-device positions was possible, which was necessary since we are showing a variable workcell, where changes can and will always happen.&lt;/p&gt;</description>
    </item>
    <item>
      <title> *Project*: BlenderProc: A procedural Blender pipeline for photorealistic rendering. (&gt;3k Stars on GitHub!)</title>
      <link>https://markusknauer.github.io/posts/blenderproc-project/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://markusknauer.github.io/posts/blenderproc-project/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;&#xA;&lt;img src=&#34;https://markusknauer.github.io/images/blenderproc.gif&#34; alt=&#34;Blenderproc&#34; width=100%&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;Very popular &lt;strong&gt;open source&lt;/strong&gt; project for fast creation of training images for &lt;strong&gt;deep neural networks&lt;/strong&gt; in &lt;strong&gt;computer vision&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;For more information please follow the link to the GitHub project:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/DLR-RM/BlenderProc&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/DLR-RM/BlenderProc&lt;/a&gt; (&amp;gt;3k stars)&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title> *Published Dataset*: &#34;HOWS-CL-25: Household Objects Within Simulation Dataset for Continual Learning&#34;  </title>
      <link>https://markusknauer.github.io/posts/hows-cl-25-dataset/</link>
      <pubDate>Sun, 23 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://markusknauer.github.io/posts/hows-cl-25-dataset/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt; created with &lt;strong&gt;BlenderProc&lt;/strong&gt;, which includes 150,795 unique synthetic images using 25 different household categories with 925 3D models.&lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt;&#xA;  &lt;img src=&#34;https://markusknauer.github.io/images/hows_overview.jpg&#34; width=&#34;500&#34;&gt;&#xA;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zenodo.org/records/7189434&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link to Dataset&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/9981968&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on IEEE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.14774&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on ArXiv&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://elib.dlr.de/190097/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on Elib&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/P9buxiinVeI&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube Link&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;For more information please follow the link to the GitHub project:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/DLR-RM/RECALL&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/DLR-RM/RECALL&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>*IROS Paper*: &#34;RECALL: Rehearsal-free Continual Learning for Object Classification&#34; </title>
      <link>https://markusknauer.github.io/posts/recall-continual-learning-iros/</link>
      <pubDate>Sun, 23 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://markusknauer.github.io/posts/recall-continual-learning-iros/</guid>
      <description>&lt;p&gt;Our approach for &lt;strong&gt;rehearsal-free&lt;/strong&gt; &lt;strong&gt;object classification&lt;/strong&gt; in a &lt;strong&gt;continual/incremental&lt;/strong&gt; fashion with &lt;strong&gt;SOTA&lt;/strong&gt; results was published in &lt;strong&gt;IEEE IROS&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;&#xA;      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/P9buxiinVeI?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;&#xA;    &lt;/div&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/9981968&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on IEEE&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.14774&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on ArXiv&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://elib.dlr.de/190097/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on Elib&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://youtu.be/P9buxiinVeI&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube Link&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;For more information please follow the link to the GitHub project:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/DLR-RM/RECALL&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/DLR-RM/RECALL&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
