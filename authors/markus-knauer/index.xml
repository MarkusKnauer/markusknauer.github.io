<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Markus Knauer</title>
    <link>https://markusknauer.github.io/authors/markus-knauer/</link>
    <description>Recent content on Markus Knauer</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 09 Aug 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://markusknauer.github.io/authors/markus-knauer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>**Summer Schools 2025:** ELLIS Cambridge &amp; OxML Oxford</title>
      <link>https://markusknauer.github.io/posts/summer-schools-2025/</link>
      <pubDate>Sat, 09 Aug 2025 00:00:00 +0000</pubDate>
      <guid>https://markusknauer.github.io/posts/summer-schools-2025/</guid>
      <description>&lt;p&gt;This summer, I had the incredible opportunity to attend two outstanding summer schools that significantly expanded my knowledge in machine learning and AI. Here&amp;rsquo;s my experience from both events.&lt;/p&gt;&#xA;&lt;h2 id=&#34;ellis-probabilistic-machine-learning-summer-school---cambridge-july-14-18-2025&#34;&gt;&#xA;  &lt;img src=&#34;https://markusknauer.github.io/images/cambridge_logo.png&#34; alt=&#34;Cambridge University Logo&#34; class=&#34;heading-logo&#34; loading=&#34;lazy&#34;&gt; ELLIS Probabilistic Machine Learning Summer School - Cambridge (July 14-18, 2025)&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#ellis-probabilistic-machine-learning-summer-school---cambridge-july-14-18-2025&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;The &lt;strong&gt;&lt;a href=&#34;https://www.ellis.eng.cam.ac.uk/summer-school/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cambridge ELLIS Unit Summer School on Probabilistic Machine Learning&lt;/a&gt;&lt;/strong&gt; was an intensive deep dive into the foundations and cutting-edge developments in probabilistic machine learning. The program took place from July 14-18, 2025, at Pembroke College, Cambridge, featuring world-class lecturers and researchers.&lt;/p&gt;</description>
    </item>
    <item>
      <title>**Automatica 2025:** DLR Showcasing the Future of Robotics</title>
      <link>https://markusknauer.github.io/posts/dlr-automatica-2025/</link>
      <pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://markusknauer.github.io/posts/dlr-automatica-2025/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://automatica-munich.com/en/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Automatica 2025&lt;/a&gt; in Munich showcased the latest advances in robotics and automation technology. DLR made a strong impression with our expansive booth spanning two areas in Hall B4 (Stand 321/324), where we demonstrated cutting-edge robotics and AI integration.&lt;/p&gt;&#xA;&lt;h2 id=&#34;dlrs-robotics-showcase&#34;&gt;&#xA;  DLR&amp;rsquo;s Robotics Showcase&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#dlrs-robotics-showcase&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Our Institute of Robotics and Mechatronics presented five groundbreaking systems:&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.dlr.de/de/rm/forschung/robotersysteme/humanoide/toro&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TORO&lt;/a&gt;&lt;/strong&gt; - Our bipedal humanoid robot demonstrates advanced locomotion, climbing stairs and navigating obstacles with enhanced perception capabilities.&lt;/p&gt;</description>
    </item>
    <item>
      <title>**ICRA 2025:** RACCOON â€” Grounding Embodied Question-Answering in Robotics</title>
      <link>https://markusknauer.github.io/posts/icra-rag-embodied-qa/</link>
      <pubDate>Mon, 28 Apr 2025 08:59:00 +0200</pubDate>
      <guid>https://markusknauer.github.io/posts/icra-rag-embodied-qa/</guid>
      <description>&lt;p&gt;Our approach for &lt;strong&gt;truthful&lt;/strong&gt; &lt;strong&gt;foundation model&lt;/strong&gt; (&lt;strong&gt;LLM&lt;/strong&gt;) question-answering using &lt;em&gt;RAG&lt;/em&gt; methods in &lt;strong&gt;robotics&lt;/strong&gt; was accepted by &lt;strong&gt;IEEE ICRA&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;&#xA;      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/OX-dd5PiiZo?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;&#xA;    &lt;/div&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/11127843/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on IEEE&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://elib.dlr.de/205203/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on Elib&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://youtu.be/OX-dd5PiiZo?si=oB5V8kO_cFpCnSgW&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube Link&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>**RA-L 2025:** Interactive Incremental Learning of Generalizable Robot Skills</title>
      <link>https://markusknauer.github.io/posts/ral-interactive-incremental-learning/</link>
      <pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://markusknauer.github.io/posts/ral-interactive-incremental-learning/</guid>
      <description>&lt;p&gt;Our approach on probabilistic machine learning method for &lt;strong&gt;interactive robot skill modulation&lt;/strong&gt; using a &lt;strong&gt;task-parameterized,&lt;/strong&gt; &lt;strong&gt;kernelized&lt;/strong&gt; method (TP-KMP) got published in IEEE Robotics and Automation Letters (&lt;strong&gt;RA-L&lt;/strong&gt;).&lt;/p&gt;&#xA;&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;&#xA;      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/nqigz0l1syA?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;&#xA;    &lt;/div&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/10887119/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on IEEE&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2409.05655&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on ArXiv&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://elib.dlr.de/212796/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on Elib&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://youtu.be/nqigz0l1syA&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube Link&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;For more information please follow the link to the GitHub project:&lt;/p&gt;</description>
    </item>
    <item>
      <title>**CoRL 2024:** Spotlight Presentation at CoRoboLearn Workshop</title>
      <link>https://markusknauer.github.io/posts/corl-2024-corobolearn-workshop/</link>
      <pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://markusknauer.github.io/posts/corl-2024-corobolearn-workshop/</guid>
      <description>&lt;p&gt;I presented two papers at the &lt;a href=&#34;https://sites.google.com/view/corobolearn&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CoRoboLearn: Advancing Learning for Human-Centered Collaborative Robots&lt;/a&gt; workshop at the &lt;a href=&#34;https://www.corl.org/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Conference on Robot Learning (CoRL) 2024&lt;/a&gt; in Munich, Germany.&lt;/p&gt;&#xA;&lt;p&gt;Our first-author paper on &lt;strong&gt;interactive incremental skill learning&lt;/strong&gt; was selected for a &lt;strong&gt;spotlight presentation&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;h3 id=&#34;spotlight-interactive-incremental-imitation-learning-of-generalizable-skills-with-local-trajectory-modulation&#34;&gt;&#xA;  Spotlight: Interactive Incremental Imitation Learning of Generalizable Skills with Local Trajectory Modulation&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#spotlight-interactive-incremental-imitation-learning-of-generalizable-skills-with-local-trajectory-modulation&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Knauer, M.&lt;/strong&gt;, Albu-SchÃ¤ffer, A., Stulp, F., SilvÃ©rio, J.&lt;/p&gt;&#xA;&lt;p&gt;We present a method for interactive, incremental learning of generalizable robot manipulation skills using task-parameterized kernelized movement primitives. Physical corrections transfer across task configurations through local trajectory modulation, and uncertainty-driven variable impedance ensures safe interaction.&lt;/p&gt;</description>
    </item>
    <item>
      <title>**RSS 2024:** Embodied Question-Answering at GenAI-HRI Workshop</title>
      <link>https://markusknauer.github.io/posts/rss-2024-genai-hri-workshop/</link>
      <pubDate>Mon, 15 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://markusknauer.github.io/posts/rss-2024-genai-hri-workshop/</guid>
      <description>&lt;p&gt;We presented our work on &lt;strong&gt;grounded embodied question-answering&lt;/strong&gt; at the &lt;a href=&#34;https://sites.google.com/view/gai-hri-2024&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generative Modeling meets HRI (GenAI-HRI)&lt;/a&gt; workshop at &lt;a href=&#34;https://roboticsconference.org/2024/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Robotics: Science and Systems (RSS) 2024&lt;/a&gt; in Delft, Netherlands.&lt;/p&gt;&#xA;&lt;p&gt;Our approach uses &lt;strong&gt;RAG&lt;/strong&gt; (Retrieval-Augmented Generation) methods to ground &lt;strong&gt;foundation model&lt;/strong&gt; (&lt;strong&gt;LLM&lt;/strong&gt;) responses in actual robot state summaries from existing robot modules, enabling &lt;strong&gt;truthful&lt;/strong&gt; question-answering about the robot&amp;rsquo;s current situation and capabilities.&lt;/p&gt;&#xA;&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;&#xA;      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/OX-dd5PiiZo?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;&#xA;    &lt;/div&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://elib.dlr.de/205203/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on Elib&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://youtu.be/OX-dd5PiiZo?si=oB5V8kO_cFpCnSgW&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube Link&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/gai-hri-2024&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GenAI-HRI Workshop&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>**Automatica 2023 &amp; Hannover Fair 2024:** Task-Parameterized Robot Demo</title>
      <link>https://markusknauer.github.io/posts/automatica-hannover-demo/</link>
      <pubDate>Mon, 22 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://markusknauer.github.io/posts/automatica-hannover-demo/</guid>
      <description>&lt;h2 id=&#34;automatica-2023--hannover-fair-2024&#34;&gt;&#xA;  Automatica 2023 &amp;amp; Hannover fair 2024&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#automatica-2023--hannover-fair-2024&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;picture class=&#34;demo-image&#34;&gt;&#xA;  &lt;source srcset=&#34;https://markusknauer.github.io/images/tradefair.webp&#34; type=&#34;image/webp&#34;&gt;&#xA;  &lt;img src=&#34;https://markusknauer.github.io/images/tradefair.jpeg&#34; alt=&#34;Robotics demonstration at Automatica and Hannover Fair&#34; loading=&#34;lazy&#34;&gt;&#xA;&lt;/picture&gt;&#xA;&lt;p&gt;Presenting a robotic demonstrator at a trade fair is always exciting! ðŸš€&lt;/p&gt;&#xA;&lt;p&gt;At &lt;a href=&#34;https://factory-of-the-future.dlr.de/2023/05/31/27-30-juni-2023-automatica-2023/index.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Automatica 2023&lt;/a&gt; in Munich and &lt;a href=&#34;https://www.dlr.de/de/aktuelles/nachrichten/2024/das-dlr-auf-der-hannover-messe-2024&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hannover fair&lt;/a&gt; 2024 in Hannover I presented an implementation of a Task-Parameterized approach (TP-GMM [1])&#xA;at the booth of the &lt;a href=&#34;https://www.dlr.de/rm&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Institute of Robotics and Mechatronics&lt;/a&gt; of the &lt;a href=&#34;https://www.dlr.de&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;German Aerospace Center (DLR)&lt;/a&gt; in the domain of future manufacturing.&#xA;The demonstration showed a ring-measuring task, where the robot arm picks a bearing ring from a box and places it onto a measuring device. Thanks to the task-parameterized approach a generalization to different box and measuring-device positions was possible, which was necessary since we are showing a variable workcell, where changes can and will always happen.&lt;/p&gt;</description>
    </item>
    <item>
      <title>**Project:** BlenderProc â€” Photorealistic Rendering for Computer Vision (3.4k&#43; GitHub Stars)</title>
      <link>https://markusknauer.github.io/posts/blenderproc-project/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://markusknauer.github.io/posts/blenderproc-project/</guid>
      <description>&lt;img src=&#34;https://markusknauer.github.io/images/blenderproc.gif&#34; alt=&#34;BlenderProc procedural pipeline generating photorealistic training images for deep learning&#34; loading=&#34;lazy&#34; style=&#34;width: 100%; display: block; margin: 0 auto;&#34;&gt;&#xA;&lt;p&gt;Very popular &lt;strong&gt;open source&lt;/strong&gt; project for fast creation of training images for &lt;strong&gt;deep neural networks&lt;/strong&gt; in &lt;strong&gt;computer vision&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;p&gt;For more information please follow the link to the GitHub project:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/DLR-RM/BlenderProc&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/DLR-RM/BlenderProc&lt;/a&gt; (&amp;gt;3.4k stars)&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>**Dataset:** HOWS-CL-25 â€” A Continual Learning Dataset for Household Objects</title>
      <link>https://markusknauer.github.io/posts/hows-cl-25-dataset/</link>
      <pubDate>Sun, 23 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://markusknauer.github.io/posts/hows-cl-25-dataset/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt; created with &lt;strong&gt;BlenderProc&lt;/strong&gt;, which includes 150,795 unique synthetic images using 25 different household categories with 925 3D models.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;picture class=&#34;demo-image&#34;&gt;&#xA;  &lt;source srcset=&#34;https://markusknauer.github.io/images/hows_overview.webp&#34; type=&#34;image/webp&#34;&gt;&#xA;  &lt;img src=&#34;https://markusknauer.github.io/images/hows_overview.jpg&#34; alt=&#34;Overview of the HOWS-CL-25 dataset showing examples from 25 household object categories&#34; loading=&#34;lazy&#34;&gt;&#xA;&lt;/picture&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://zenodo.org/records/7189434&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link to Dataset&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/9981968&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on IEEE&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.14774&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on ArXiv&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://elib.dlr.de/190097/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on Elib&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/P9buxiinVeI&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube Link&lt;/a&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;For more information please follow the link to the GitHub project:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/DLR-RM/RECALL&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/DLR-RM/RECALL&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>**IROS 2022:** RECALL â€” Rehearsal-Free Continual Learning for Object Classification</title>
      <link>https://markusknauer.github.io/posts/recall-continual-learning-iros/</link>
      <pubDate>Sun, 23 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://markusknauer.github.io/posts/recall-continual-learning-iros/</guid>
      <description>&lt;p&gt;Our approach for &lt;strong&gt;rehearsal-free&lt;/strong&gt; &lt;strong&gt;object classification&lt;/strong&gt; in a &lt;strong&gt;continual/incremental&lt;/strong&gt; fashion with &lt;strong&gt;SOTA&lt;/strong&gt; results was published in &lt;strong&gt;IEEE IROS&lt;/strong&gt;.&lt;/p&gt;&#xA;&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;&#xA;      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/P9buxiinVeI?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;&#xA;    &lt;/div&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/9981968&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on IEEE&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.14774&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on ArXiv&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://elib.dlr.de/190097/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper on Elib&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://youtu.be/P9buxiinVeI&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YouTube Link&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;For more information please follow the link to the GitHub project:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/DLR-RM/RECALL&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/DLR-RM/RECALL&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
